{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backpropagation](backpropagation1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function: Q(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "sigmoid(3)\n",
    "\n",
    "\n",
    "# Sigmoid Derivative: Q'(x) = Q(x) * (1 - Q(x))\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid(sigmoid_output) * (1 - sigmoid(sigmoid_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid Function (Activation Function) \n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The derivative of the sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "- np.exp(x) -> it means ${e^x}$, where e is Euler's number (approximately equal to 2.71828).\n",
    "- np.exp supports both scalar and array inputs.\n",
    "- math.exp(x) -> it also means ${e^x}$, but only supports scalar values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "x1 = 0.5\n",
    "x2 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected output\n",
    "y_expected = 1\n",
    "\n",
    "# learning rate\n",
    "n = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "\n",
    "w1 = 0.7010\n",
    "w2 = 0.3009\n",
    "w3 = 0.4011\n",
    "w4 = 0.6005\n",
    "w5 = 0.551\n",
    "w6 = 0.4595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Architecture:\n",
    "- Inputs: $x_1$, $x_2$\n",
    "- Hidden Layer: Two neurons ($h_1$, $h_2$)\n",
    "- Output Layer: Single neuron\n",
    "\n",
    "- Forward Propagation Formulas:\n",
    "- First Hidden Neuron: \n",
    "$$ z_1 = x_1 \\cdot w_1 + x_2 \\cdot w_3 $$\n",
    " $$ h_1 = \\sigma(z_1) $$\n",
    "\n",
    " - Second Hidden Neuron:\n",
    "$$ z_2 = x_1 \\cdot w_2 + x_2 \\cdot w_4 $$\n",
    "$$ h_2 = \\sigma(z_2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurons and activation function step\n",
    "def l1f1(x1,x2):\n",
    "    l1f1_result = x1*w1 + x2*w3 # z1 step\n",
    "    return sigmoid(l1f1_result) # h1 step, activation func applied\n",
    "\n",
    "def l1f2(x1,x2):\n",
    "    l1f2_result = x1*w2 + x2*w4 #z2 step\n",
    "    return sigmoid(l1f2_result) # h2 step, activation func applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "def of1(h1,h2):\n",
    "    of1_result = h1*w5 + h2*w6 # z3 step\n",
    "    return sigmoid(of1_result) # h3 step, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1:  0.616\n",
      "h2:  0.582\n"
     ]
    }
   ],
   "source": [
    "h1 = l1f1(x1,x2)\n",
    "h2 = l1f2(x1,x2)\n",
    "\n",
    "print(\"h1: \", round(h1,3))\n",
    "print(\"h2: \", round(h2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1:  0.647\n"
     ]
    }
   ],
   "source": [
    "output1 = of1(h1, h2)\n",
    "print(\"output1: \", round(output1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost / Cost / Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means Squared Error\n",
    "def mse(y_expected, y_predicted):\n",
    "    return (y_expected - y_predicted) ** 2 # y is th e expected output, y_hat is the predicted output\n",
    "\n",
    "# MSE Derivative\n",
    "def mse_derivative(y_expected, y_predicted):\n",
    "    return -2 * (y_expected - y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error (MSE) is a commonly used loss function in machine learning. \n",
    "The formula for MSE is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_{\\text{expected}, i} - y_{\\text{predicted}, i})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the total number of samples.\n",
    "\n",
    "\n",
    "The Mean Squared Error (MSE) is defined as:\n",
    "\n",
    "The derivative of MSE with respect to y_predicted is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial y_{\\text{predicted}}} = -\\frac{2}{N} (y_{\\text{expected}} - y_{\\text{predicted}})\n",
    "$$\n",
    "\n",
    "\n",
    "For simplicity, if we omit the $\\frac{1}{N}$ term (e.g., for a single training example or simplicity in implementation), the derivative becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{MSE}}{\\partial y_{\\text{predicted}}} = -2 (y_{\\text{expected}} - y_{\\text{predicted}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In our case: Simplification in backpropagation\n",
    "- In backpropagation, the key goal is to compute the gradient of the loss with respect to the weights.\n",
    "\n",
    "- We did not include $\\frac{1}{N}$ factor in the case of MSE because our example is simple, so it does not affect the result much.\n",
    "\n",
    "- In larger neural networks with multiple examples, $\\frac{1}{N}$ to average the gradient, which helps make the weight updates more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.124\n",
      "Expected:  1 | Predicted:  0.647\n"
     ]
    }
   ],
   "source": [
    "first_mse = mse(y_expected, output1)\n",
    "print(\"MSE: \", round(first_mse, 3))\n",
    "print(\"Expected: \", y_expected, \"|\", \"Predicted: \", round(output1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_o1:  -0.706\n"
     ]
    }
   ],
   "source": [
    "# Derivative of the loss function with respect to the output layer (y_predicted)\n",
    "dL_o1 = mse_derivative(y_expected, output1) \n",
    "print(\"dL_o1: \", round(dL_o1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the Loss w.r.t. Output $o_1$\n",
    "\n",
    "Derivative of the loss with respect to $o_1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial o_1} = -2 \\cdot (y_{\\text{expected}} - o_1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y_{\\text{expected}}$ is the true value.\n",
    "- $o_1$ is the predicted output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do1_z3:  0.221\n"
     ]
    }
   ],
   "source": [
    "# Gradient of the loss wrt output\n",
    "do1_z3 = sigmoid_derivative(dL_o1)\n",
    "print(\"do1_z3: \", round(do1_z3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient of Output w.r.t. ${z_3}$ (Pre-activation Output)\n",
    "\n",
    "Using the sigmoid derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial o_1}{\\partial z_3} = \\sigma(z_3) \\cdot (1 - \\sigma(z_3))\n",
    "$$\n",
    "\n",
    "- Chain Rule: Loss Gradient w.r.t. \\(z_3\\)\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_3} = \\frac{\\partial L}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden Layers: ${H_2}$ and ${H_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw5:  -0.096\n",
      "dw6:  -0.091\n"
     ]
    }
   ],
   "source": [
    "dw5 = dL_o1 * do1_z3 * h1\n",
    "dw6 = dL_o1 * do1_z3 * h2\n",
    "print(\"dw5: \", round(dw5, 3))\n",
    "print(\"dw6: \", round(dw6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the Output Layer Weights\n",
    "\n",
    "The gradient for $ w_5 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_5 = \\frac{\\partial L}{\\partial w_5} = \\frac{\\partial h_3}{\\partial L} \\cdot \\frac{\\partial z_3}{\\partial h_3} \\cdot \\frac{\\partial w_5}{\\partial z_3} = \\text{dL}_{o1} \\cdot \\text{do1}_{z3} \\cdot h_1\n",
    "$$\n",
    "\n",
    "The gradient for $ w_6 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_6 = \\frac{\\partial L}{\\partial w_6} = \\text{dL}_{o1} \\cdot \\text{do1}_{z3} \\cdot h_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for hidden layer\n",
    "dh1_z1 = sigmoid_derivative(h1)\n",
    "dh2_z2 = sigmoid_derivative(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Backpropagation to hidden layer\n",
    "dL_h1 = dL_o1 * do1_z3 * w5\n",
    "dL_h2 = dL_o1 * do1_z3 * w6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of the Input Layer Weights\n",
    "\n",
    "The gradient for $ w_1 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_1 = \\frac{\\partial L}{\\partial w_1} = \\text{dL}_{h1} \\cdot \\text{dh1}_{z1} \\cdot x_1\n",
    "$$\n",
    "\n",
    "The gradient for $ w_2 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_2 = \\frac{\\partial L}{\\partial w_2} = \\text{dL}_{h2} \\cdot \\text{dh2}_{z2} \\cdot x_1\n",
    "$$\n",
    "\n",
    "The gradient for $ w_3 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_3 = \\frac{\\partial L}{\\partial w_3} = \\text{dL}_{h1} \\cdot \\text{dh1}_{z1} \\cdot x_2\n",
    "$$\n",
    "\n",
    "The gradient for $ w_4 $ is calculated as:\n",
    "\n",
    "$$\n",
    "\\Delta w_4 = \\frac{\\partial L}{\\partial w_4} = \\text{dL}_{h2} \\cdot \\text{dh2}_{z2} \\cdot x_2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw1:  -0.01\n",
      "dw2:  -0.008\n",
      "dw3:  -0.006\n",
      "dw4:  -0.005\n"
     ]
    }
   ],
   "source": [
    "# Gradients for input layer weights\n",
    "dw1 = dL_h1 * dh1_z1 * x1\n",
    "dw2 = dL_h2 * dh2_z2 * x1\n",
    "dw3 = dL_h1 * dh1_z1 * x2\n",
    "dw4 = dL_h2 * dh2_z2 * x2\n",
    "\n",
    "print(\"dw1: \", round(dw1, 3))\n",
    "print(\"dw2: \", round(dw2, 3))\n",
    "print(\"dw3: \", round(dw3, 3))\n",
    "print(\"dw4: \", round(dw4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Update Rule\n",
    "\n",
    "The weight update rule is given by:\n",
    "\n",
    "$$\n",
    "w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\Delta w\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_new = w1 - n * dw1\n",
    "w2_new = w2 - n * dw2\n",
    "w3_new = w3 - n * dw3\n",
    "w4_new = w4 - n * dw4\n",
    "w5_new = w5 - n * dw5\n",
    "w6_new = w6 - n * dw6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Weights:\n",
      "w1:  0.702\n",
      "w2:  0.3017\n",
      "w3:  0.4017\n",
      "w4:  0.601\n",
      "w5:  0.5606\n",
      "w6:  0.4686\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUpdated Weights:\")\n",
    "print(\"w1: \", round(w1_new, 4))\n",
    "print(\"w2: \", round(w2_new, 4))\n",
    "print(\"w3: \", round(w3_new, 4))\n",
    "print(\"w4: \", round(w4_new, 4))\n",
    "print(\"w5: \", round(w5_new, 4))\n",
    "print(\"w6: \", round(w6_new, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Weight Update:\n",
      "h1:  0.616\n",
      "h2:  0.582\n",
      "output1:  0.65\n"
     ]
    }
   ],
   "source": [
    "# Re-run forward propagation with updated weights\n",
    "def l1f1_new(x1, x2):\n",
    "    l1f1_result = x1*w1_new + x2*w3_new\n",
    "    return sigmoid(l1f1_result)\n",
    "\n",
    "def l1f2_new(x1, x2):\n",
    "    l1f2_result = x1*w2_new + x2*w4_new\n",
    "    return sigmoid(l1f2_result)\n",
    "\n",
    "def of1_new(h1, h2):\n",
    "    of1_result = h1*w5_new + h2*w6_new\n",
    "    return sigmoid(of1_result)\n",
    "\n",
    "h1_new = l1f1_new(x1, x2)\n",
    "h2_new = l1f2_new(x1, x2)\n",
    "output1_new = of1_new(h1_new, h2_new)\n",
    "\n",
    "print(\"\\nAfter Weight Update:\")\n",
    "print(\"h1: \", round(h1_new, 3))\n",
    "print(\"h2: \", round(h2_new, 3))\n",
    "print(\"output1: \", round(output1_new, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New MSE:  0.123\n"
     ]
    }
   ],
   "source": [
    "# Recalculate MSE\n",
    "new_mse = mse(y_expected, output1_new)\n",
    "print(\"New MSE: \", round(new_mse, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
