{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src="backpropagation1.png" alt="example1" style="width:50%; height:50%;">"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function: Q(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "\n",
    "sigmoid(3)\n",
    "\n",
    "\n",
    "# Sigmoid Derivative: Q'(x) = Q(x) * (1 - Q(x))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- np.exp(x) -> it means e^x, where e is Euler's number (approximately equal to 2.71828)\n",
    "- np.exp supports both scalar and array inputs\n",
    "- math.exp(x) -> it also means e^x, but only supports scalar values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "x1 = 0.5\n",
    "x2 = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected output\n",
    "y_expected = 1\n",
    "\n",
    "# learning rate\n",
    "n = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "\n",
    "w1 = 0.7010\n",
    "w2 = 0.3009\n",
    "w3 = 0.4011\n",
    "w4 = 0.6005\n",
    "w5 = 0.551\n",
    "w6 = 0.4595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurons and activation function step\n",
    "def l1f1(x1,x2):\n",
    "    l1f1_result = x1*w1 + x2*w3 # z1 step\n",
    "    return sigmoid(l1f1_result) # h1 step, activation func applied\n",
    "\n",
    "def l1f2(x1,x2):\n",
    "    l1f2_result = x1*w2 + x2*w4 #z2 step\n",
    "    return sigmoid(l1f2_result) # h2 step, activation func applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "def of1(h1,h2):\n",
    "    of1_result = h1*w5 + h2*w6 # z3 step\n",
    "    return sigmoid(of1_result) # h3 step, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1:  0.616\n",
      "h2:  0.582\n"
     ]
    }
   ],
   "source": [
    "h1 = l1f1(x1,x2)\n",
    "h2 = l1f2(x1,x2)\n",
    "\n",
    "print(\"h1: \", round(h1,3))\n",
    "print(\"h2: \", round(h2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1:  0.647\n"
     ]
    }
   ],
   "source": [
    "output1 = of1(h1, h2)\n",
    "print(\"output1: \", round(output1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lost / Cost / Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Means Squared Error\n",
    "def mse(y_expected, y_predicted):\n",
    "    return (y_expected - y_predicted) ** 2 # y is the expected output, y_hat is the predicted output\n",
    "\n",
    "# MSE Derivative\n",
    "def mse_derivative(y_expected, y_predicted):\n",
    "    return 2 * (y_predicted - y_expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error (MSE) is a commonly used loss function in machine learning. \n",
    "The formula for MSE is:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_{\\text{expected}, i} - y_{\\text{predicted}, i})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( N \\) is the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In our case: Simplification in backpropagation\n",
    "- In backpropagation, the key goal is to compute the gradient of the loss with respect to the weights.\n",
    "\n",
    "- We did not include 1/N factor in the case of MSE because our example is simple, so it does not affect the result much.\n",
    "\n",
    "- In larger neural networks with multiple examples, 1/N to average the gradient, which helps make the weight updates more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.124\n"
     ]
    }
   ],
   "source": [
    "first_mse = mse(y_expected, output1)\n",
    "print(\"MSE: \", round(first_mse, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
